<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Columbia University EECS E4764 IoT Project Report #21</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<!-- <span class="image avatar48"><img src="images/avatar.jpg" alt="" /></span> -->
							<h1 id="title">Ghost Arm</h1>
							<p>Columbia University <br>
								EECS E4764 Fall'25 <br>
								Artificial Intelligence of Things<br>
								Team 21 Project Report
							</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<!--

								Prologue's nav expects links in one of two formats:

								1. Hash link (scrolls to a different section within the page)

								   <li><a href="#foobar" id="foobar-link" class="icon fa-whatever-icon-you-want skel-layers-ignoreHref"><span class="label">Foobar</span></a></li>

								2. Standard link (sends the user to another page/site)

								   <li><a href="http://foobar.tld" id="foobar-link" class="icon fa-whatever-icon-you-want"><span class="label">Foobar</span></a></li>

							-->
							<ul>
								<li><a href="#top" id="top-link" class="skel-layers-ignoreHref"><span class="icon fa-home">Abstract</span></a></li>
								<li><a href="#motivation" id="motivation-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Motivation</span></a></li>
								<li><a href="#system" id="system-link" class="skel-layers-ignoreHref"><span class="icon fa-th">System</span></a></li>
								<li><a href="#results" id="results-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Results</span></a></li>
								<li><a href="#references" id="references-link" class="skel-layers-ignoreHref"><span class="icon fa-th">References</span></a></li>
								<li><a href="#team" id="team-link" class="skel-layers-ignoreHref"><span class="icon fa-user">Our Team</span></a></li>
								<li><a href="#contact" id="contact-link" class="skel-layers-ignoreHref"><span class="icon fa-envelope">Contact</span></a></li>
							</ul>
						</nav>

				</div>

				<div class="bottom">

					<!-- Social Icons -->
						<ul class="icons">
							<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon fa-github"><span class="label">Github</span></a></li>
							<li><a href="#" class="icon fa-dribbble"><span class="label">Dribbble</span></a></li>
							<li><a href="#" class="icon fa-envelope"><span class="label">Email</span></a></li>
						</ul>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<iframe
								width="560"
								height="315"
								src="https://www.youtube.com/embed/jjGkz3fGHuc"
								title="GhostArm Demo (It might not open but feel free to click the YouTube link)"
								frameborder="0"
								allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
								allowfullscreen>
							</iframe>

							<h2 class="alt">Ghost Arm</h2>

							<p align="left">
								GhostArm is a real-time gesture recognition and robotic control system built entirely on low-power,
								edge-compute hardware. Using a multimodal sensor array comprised of a 3-axis IMU, an 8×8 thermal camera,
								and dual ultrasonic distance sensors, the system captures human arm movements and interprets them through
								a unified deep learning pipeline. Each sensor stream is temporally synchronized, encoded using
								lightweight convolutional networks, and fused within a Transformer-based temporal model. The final
								classifier runs locally on a Raspberry Pi, enabling low-latency gesture recognition without reliance
								on cloud services. GhostArm demonstrates that expressive human–robot interaction can be achieved using
								inexpensive sensors, efficient models, and edge-native machine learning, making the system suitable for
								remote control, assistive robotics, and constrained environments where privacy, bandwidth, and power
								are critical limitations.
							</p>

							<footer>
								<a href="#motivation" class="button scrolly">Motivation</a>
							</footer>

						</div>
					</section>


				<!-- Portfolio -->
					<section id="motivation" class="two">
						<div class="container">

							<header>
								<h2>Motivation</h2>
							</header>

							<p align="left">The motivation behind GhostArm is three-fold. The first and main motivation of this project is to push the capabilities of modern edge compute and ml stack. We aim to answer the question, leveraging low power sensors and models that can run natively on the edge, can we create efficient remote control systems? As it is based on the edge device RaspBerry Pi, using sensors such as accelerometers, thermal sensors and ultrasound sensors, we are aiming to show that such a low power system can work. And finally, robot arms are cool. do we really need a reason better than that?  </p>



						</div>
					</section>


					<section id="system" class="three">
						<div class="container">

							<header>
								<h2>System</h2>
							</header>

							<h3 align="left">Architecture</h3>

							<p align="left">Our architecture is shown in the block diagram below. </p>
							<article class="item">
								<a href="#" class="image fit"><img src="images/block_diagram.png" alt="" /></a>
								<header>
									<h3>High level architecture of GhostArm</h3>
								</header>
							</article>
							<p align="left">The block diagram illustrates the key components and their interactions within the GhostArm system.<br />
							The system consists of the following main components:
							<div style="text-align: left;">
								<ul>
									<li>
										<strong>Input Sensors:</strong>
										These sensors capture the user's arm movements and gestures. They include
										accelerometers, ultrasound sensors, and thermal sensors.
									</li>

									<li>
										<strong>Processing Unit:</strong>
										The main control unit is the Raspberry Pi. It processes sensor data in real time,
										runs the machine learning models that interpret the user's movements, and
										communicates with the robotic arm to send movement commands.
									</li>

									<li>
										<strong>Machine Learning Models:</strong>
										Our gesture recognition pipeline uses a multimodal deep learning architecture
										designed for time-series fusion. Raw data from the IMU and ultrasonic sensors
										is processed using lightweight 1D convolutional encoders, while the thermal
										sensor sequence is encoded using a compact 2D CNN. All modality embeddings
										are fused into a single temporal sequence and passed to a Transformer encoder.
										We selected the Transformer over traditional recurrent models because its
										self-attention mechanism can model long-range temporal structure and
										inter-sensor relationships more effectively, especially when each modality
										updates at different sampling rates. This enables more robust gesture
										recognition under real-world variability. The resulting model is optimized
										to run on-device with low latency, allowing the Raspberry Pi to classify
										gestures in real time without cloud resources.
									</li>

									<li>
										<strong>Robotic Arm:</strong>
										The physical component that executes commands received from the processing
										unit, mimicking the user's arm movements based on the interpreted data.
									</li>

									<li>
										<strong>Cloud:</strong>
										The cloud component stores user data and provides additional processing power
										when needed, such as during offline model training or large-scale experiments.
									</li>
								</ul>
							</div>


							<h3 align="left">Technical Components</h3>

							<div style="text-align: left;">

								<p>The purpose and purchase links for each technical component are outlined as follows:</p>

								<ol>
									<li><strong>Robot Arm (KEYESTUDIO 4DOF Arduino Robot):</strong> Controlled via four servo motors and commanded over UART from the Raspberry Pi.</li>
									<li><strong>IR Imager (AMG8833 8×8 Thermal Camera):</strong> Captures heat-based spatial changes for gesture recognition.</li>
									<li><strong>Ultrasonic Sensors (HC-SR04, two units):</strong> Detects depth and lateral motion using distance measurements.</li>
									<li><strong>Accelerometer (ADXL345):</strong> Measures acceleration and rotation during gestures.</li>
									<li><strong>Raspberry Pi 5:</strong> Central compute and control unit.</li>
								</ol>

								<p>Hardware setup procedure:</p>

								<ol>
									<li>Flash the Arduino using <code>RobotMC/Robot_MC_Codes/MC_ESP.ino</code>.</li>
									<li>Connect the Arduino to the Raspberry Pi via UART and test using <code>RaspberryPi/Robot.py</code>.</li>
									<li>Wire sensors to the Raspberry Pi (I2C for IMU/thermal, GPIO for ultrasonic sensors).</li>
									<li>Run <code>RaspberryPi/server/server.py</code> for live sensor visualization.</li>
								</ol>

							</div>
	
							<h3 align="left">Software Repository Structure</h3>

					<p align="left">
						The Raspberry Pi software stack is organized into modular scripts that separately handle data
						collection, real-time inference, robotic actuation, and the web-based visualization interface:
					</p>

					<div style="text-align: left;">
					<pre><code>AIoT_Final_Project/
					├── RobotMC/                           # Microcontroller-side code (ESP32 / Arduino)
					│   ├── Robot_Test_Code/               # Experimental / prototyping firmware
					│   │   ├── Robot_Test_Code.ino        # Basic robot control and sensor tests
					│   │   └── MC_ESP.ino                 # ESP32 communication and control logic
					│   │
					│   └── Robot_MC_Codes/                # Finalized microcontroller firmware
					│       ├── Robot_Test_Code.ino        # Stable robot motion and actuation code
					│       └── MC_ESP.ino                 # ESP32 firmware for serial/WiFi control
					│
					├── RasberryPi/                        # On-device computation & ML inference
					│   ├── server/                        # Backend + web dashboard
					│   │   ├── server.py                  # API server for sensor streaming and robot control
					│   │   └── index.html                 # Web-based dashboard for live visualization
					│   │
					│   ├── sensors_test/                  # test the different sensors in teh sensor array
					│   │   ├── accel_test.py
					│   │   └── thermal_test.py
					│   │   └── ultrasonic_test.py
					│   │
					│   ├── models/                        # Trained machine learning models
					│   │   └── transformer.pt             # Gesture recognition Transformer model
					|	|	└── LSTM.pt					   # Gesture recognition LSTM model
					│   │
					│   ├── data_root/                     # Labeled sensor dataset
					│   │   ├── move_left/                 # Gesture class: move left
					│   │   ├── move_right/                # Gesture class: move right
					│   │   ├── move_up/                   # Gesture class: move up
					│   │   ├── move_down/                 # Gesture class: move down
					│   │   └── ... (CSV recordings)       # Time-series sensor data files
					│   │
					│   ├── collect_samples_2.py           # Sensor data collection and labeling script
					│   ├── live_inference.py              # Real-time gesture inference pipeline
					│   └── robot_control.py               # Maps predicted gestures to robot commands
					│   └── transformer.py                 # Training the Transformer model
					│   └── LSTM.py               		   # Training the LSTM model
					│   └── sensor_data_collect.py         # Collect Sensor Data
					│
					└── README.md                          # Project overview and setup instructions
					</code></pre>

					</div>


					<p align="left">
						The dashboard periodically queries the backend for live sensor data and sends high-level control
						commands, while the backend coordinates inference and actuation in real time.
					</p>



							<h3 align="left">Prototype</h3>

							<p align="left">The prototype developed for this project demonstrates an end-to-end gesture-controlled robotic system that integrates multimodal sensing, on-device machine learning inference, and real-time actuation. A Raspberry Pi serves as the central processing unit, collecting synchronized data from an accelerometer, thermal imaging sensor, and dual ultrasonic distance sensors to capture complementary aspects of user motion. This data is processed in real time by a lightweight multimodal deep learning model optimized for embedded execution, which classifies gestures with low latency. The predicted gestures are then translated into control commands and transmitted to an Arduino-controlled robotic arm, enabling closed-loop operation entirely at the edge without reliance on cloud resources.</p>



						</div>
					</section>


					<section id="results" class="two">
						<div class="container">

							<header>
								<h2>Results</h2>
							</header>

							<p align="left">
								This section presents the performance of the two gesture recognition models evaluated in this project:
								a bidirectional LSTM-based model and a Transformer-based model. Accuracy curves are shown to illustrate
								training dynamics, convergence behavior, and generalization performance.
							</p>

							<!-- LSTM Results -->
							<article class="item">
								<a href="images/training_curve_lstm.png" class="image fit">
									<img src="images/training_curve_lstm.png" alt="LSTM Training and Validation Accuracy" />
								</a>
								<header>
									<h3>LSTM-Based Multimodal Model</h3>
								</header>
								<p align="left">
									The bidirectional LSTM model demonstrates steady learning behavior, with training and validation
									accuracy improving consistently over epochs. The relatively small gap between training and validation
									curves suggests limited overfitting and reasonable generalization to unseen data.
									This model benefits from explicit temporal modeling, making it well-suited for short, fixed-duration
									gesture sequences.
								</p>
							</article>

							<!-- Transformer Results -->
							<article class="item">
								<a href="images/transformer_training_curve.png" class="image fit">
									<img src="images/transformer_training_curve.png" alt="Transformer Training and Validation Accuracy" />
								</a>
								<header>
									<h3>Transformer-Based Multimodal Model</h3>
								</header>
								<p align="left">
									The Transformer-based model achieves higher peak validation accuracy compared to the LSTM,
									indicating improved representational capacity for multimodal sensor fusion.
									However, the wider gap between training and validation accuracy suggests a higher risk of overfitting,
									likely due to increased model complexity and the limited size of the dataset.
									This behavior highlights the importance of regularization and data scaling when using attention-based
									architectures on embedded or small-scale datasets.
								</p>
							</article>

							<p align="left">
								Overall, both models successfully learn discriminative representations from fused IMU, thermal,
								and ultrasonic data. While the Transformer offers stronger performance potential, the LSTM provides
								a more stable and computationally efficient solution, making it a compelling choice for real-time,
								on-device inference on resource-constrained platforms such as the Raspberry Pi.
							</p>

						</div>
					</section>


					<section id="references" class="three">
						<div class="container">

							<header>
								<h2>References</h2>
							</header>

							<p align="left">
								<b>[1]</b> Hochreiter, S., & Schmidhuber, J. (1997). 
								"Long Short-Term Memory." <i>Neural Computation</i>.<br><br>

								<b>[2]</b> Vaswani, A. et al. (2017). 
								"Attention Is All You Need." <i>Advances in Neural Information Processing Systems</i>.<br><br>

								<b>[3]</b> Zeng, M., Nguyen, L. T., Yu, B., Mengshoel, O. J., Zhu, J., Wu, P., & Zhang, J. (2014). 
								"Convolutional Neural Networks for Human Activity Recognition Using Mobile Sensors." 
								<i>IEEE MobiCASE</i>.<br><br>

								<b>[4]</b> Hammerla, N. Y., Halloran, S., & Plötz, T. (2016). 
								"Deep, Convolutional, and Recurrent Models for Human Activity Recognition Using Wearables." 
								<i>IJCAI</i>.<br><br>

								<b>[5]</b> Adafruit AMG8833 Thermal Camera Datasheet. 
								Adafruit Industries. 
								https://cdn-learn.adafruit.com/downloads/pdf/adafruit-amg8833-8x8-thermal-camera-sensor.pdf<br><br>

								<b>[6]</b> HC-SR04 Ultrasonic Sensor Technical Specification. 
								ElecFreaks / EEVblog. 
								https://cdn.sparkfun.com/datasheets/Sensors/Proximity/HCSR04.pdf<br><br>

								<b>[7]</b> Ravi, N. et al. (2005). 
								"Activity Recognition from Accelerometer Data." 
								<i>AAAI</i>.<br><br>

								<b>[8]</b> Lane, N. D. et al. (2015). 
								"DeepEar: Robust Smartphone Audio Sensing in Unconstrained Acoustic Environments Using Deep Learning." 
								<i>UbiComp</i>.<br><br>

								<b>[9]</b> Zhang, Z. (2012). 
								"Motions and Gestures Recognition by Multimodal Sensor Fusion." 
								<i>IEEE Sensors Journal</i>.<br><br>

								<b>[10]</b> Raspberry Pi Documentation. 
								https://www.raspberrypi.com/documentation/<br><br>

								<b>[11]</b> Paszke, A. et al. (2019). 
								"PyTorch: An Imperative Style, High-Performance Deep Learning Library." 
								<i>NeurIPS</i>.<br><br>

								<b>[12]</b> Wu, Y. et al. (2019). 
								"On-device Machine Learning: An Algorithmic and Hardware Co-design Approach." 
								<i>ACM Computing Surveys</i>.
							</p>

						</div>
					</section>



				<!-- About Me -->
					<section id="team" class="two">
						<div class="container">

							<header>
								<h2>Our Team</h2>
							</header>

							<!-- <a href="#" class="image featured"><img src="images/pic08.jpg" alt="" /></a> -->


							<div class="row">
								<div class="4u 12u$(mobile)">
									<article class="item">
										<a href="#" class="image fit"><img src="images/sri_pfp.jpg" alt="" /></a>
										<header>
											<h3>Sri Iyengar</h3>
												<p>
													I am a Master's in Computer Engineering Student graduating in December 2025. I am passionate about Deep Learning and Machine Learning. 
													<a href="https://www.linkedin.com/in/sri-iyengar" target="_blank">
														LinkedIn
													</a>.
												</p>
										</header>
									</article>
								</div>
								<div class="4u 12u$(mobile)">
									<article class="item">
										<a href="#" class="image fit"><img src="images/pic07.jpg" alt="" /></a>
										<header>
											<h3>Team Member</h3>
											<p>Description and link</p>
										</header>
									</article>
								</div>
								<div class="4u$ 12u$(mobile)">
									<article class="item">
										<a href="#" class="image fit"><img src="images/pic07.jpg" alt="" /></a>
										<header>
											<h3>Team Member</h3>
											<p>Description and link</p>
										</header>
									</article>
								</div>
							</div>

						</div>
					</section>

				<!-- Contact -->
					<section id="contact" class="four">
						<div class="container">

							<header>
								<h2>Contact</h2>
							</header>

							<p align="left">
								<strong>Sri Iyengar: </strong>si2468@columbia.edu</br>
								<strong>Diwa Bhusal: </strong>db3472@columbia.edu</br>
								<strong>Aymen Ahmed Norain: </strong>aan2161@columbia.edu</br>
								<strong>Saivignesh Venkatraman: </strong>sv2795@columbia.edu</br>
												</br>
								<strong>Columbia University </strong><a href="http://www.ee.columbia.edu">Department of Electrical Engineering</a><br>
								<!-- <strong>Class Website:</strong>
									<a href="https://edblogs.columbia.edu/eecs4764-001-2019-3/">Columbia University EECS E4764 Fall '22 IoT</a></br> -->
								<strong>Instructor:</strong> <a href="https://www.engineering.columbia.edu/faculty-staff/directory/xiaofan-fred-jiang">Professsor Xiaofan (Fred) Jiang</a>
							</p>


							<!-- <form method="post" action="#">
								<div class="row">
									<div class="6u 12u$(mobile)"><input type="text" name="name" placeholder="Name" /></div>
									<div class="6u$ 12u$(mobile)"><input type="text" name="email" placeholder="Email" /></div>
									<div class="12u$">
										<textarea name="message" placeholder="Message"></textarea>
									</div>
									<div class="12u$">
										S<input type="submit" value="Send Message" />
									</div>
								</div>
							</form> -->

						</div>
					</section>

			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; IoT Project | All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
